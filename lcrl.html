<html>
	<head>
		<title>Logically-Constrained Reinforcement Learning</title>
		<link rel="icon" href="favicon.ico">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link href="assets/css/all.css" rel="stylesheet">
		<link href="assets/css/main.css" rel="stylesheet">
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<!-- MathJax -->
		<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
		  jax: ["input/TeX", "output/HTML-CSS"],
		  extensions: ["tex2jax.js"],
		  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
		  tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
		  TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
		  messageStyle: "none"
		});
		</script>
		<!-- clustrmaps -->
		<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=272833&w=200&t=tt&d=169h2XnHo-UN9lmnCHpHTItClzsEGQegWtH9pUKo9tM&co=1c1d26&ct=808080&cmn=00ff0c&cmo=00ff0c'></script>
		
	</head>
	<body class="is-preload landing">
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1 id="logo"><a href="index.html"></a></h1>
					<nav id="nav">
						<ul>
							<li><a href="index" class="scrolly">Home</a></li>
							<li><a href="publications">Publications</a></li>
							<li><a href="index#news" class="scrolly">News</a></li>
							<li><a href="misc">Misc</a></li>
							<!-- Menu
							<li>
								<a href="#">Publications</a>
								<ul>
									<li><a href="left-sidebar.html">Left Sidebar</a></li>
									<li><a href="right-sidebar.html">Right Sidebar</a></li>
									<li><a href="no-sidebar.html">No Sidebar</a></li>
									<li>
										<a href="#">Submenu</a>
										<ul>
											<li><a href="#">Option 1</a></li>
											<li><a href="#">Option 2</a></li>
											<li><a href="#">Option 3</a></li>
											<li><a href="#">Option 4</a></li>
										</ul>
									</li>
								</ul>
							</li>
							-->
							<li><a href="#footer" class="button scrolly">Contact</a></li>
							<li><a> </a></li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<div id="main" class="wrapper style1">
					<div class="container">
						<header class="major">
							<h2>Logically-Constrained Reinforcement Learning (LCRL)</h2>
						</header>
						<section id="content">
							<h3>Description</h3>
							<p align=”justify”><h4>Logically-Constrained Reinforcement Learning (LCRL) 
							is a model-free reinforcement learning framework to synthesise policies for unknown, 
							continuous-state Markov Decision Processes (MDPs) under a given Linear Temporal Logic (LTL) property. 
							LCRL automatically shapes a synchronous reward function on-the-fly. This enables any off-the-shelf RL 
							algorithm to synthesise policies that yield traces which probabilistically satisfy the LTL property. 
							This probability is calculated in parallel with the learning process when the MDP state space is finite. 
							LCRL produces policies that are certified with respect to the LTL property.
							</h4>
							</p>
							
							<br>
							
							<h3>Code Repository</h3>
							<p align=”justify”><h4> LCRL code can be found here: <a href="https://github.com/grockious/lcrl" target="_blank"><u>github.com/grockious/lcrl <i class="fas fa-external-link-alt"></i></u></a>
							</h4>
							</p>
					
							<br>
							
							<h3>Publications</h3>
								<ul class="alt">
									<li><b>&bull; <u> Hasanbeig, M. </u>, Kroening, D. and Abate, A., "<u>Deep Reinforcement Learning with Temporal Logics</u>", International Conference on Formal Modeling and Analysis of Timed Systems, 2020. <a href="javascript:hide_f('bib_ddpg')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-030-57628-8_1.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="bib_ddpg" style="display: none; width: 80%;" rows="8">@inproceedings{hasanbeig2020deep,
title={Deep Reinforcement Learning with Temporal Logics},
author={Hasanbeig, Mohammadhosein and Kroening, Daniel and Abate, Alessandro},
booktitle={International Conference on Formal Modeling and Analysis of Timed Systems},
pages={1--22},
year={2020},
organization={Springer}
}</textarea>
									<li><b>&bull; <u> Hasanbeig, M. </u>, Abate, A. and Kroening, D., "<u>Cautious Reinforcement Learning with Logical Constraints</u>", International Conference on Autonomous Agents and Multi-agent Systems, 2020. <a href="javascript:hide_f('bib_sparl')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://arxiv.org/pdf/2002.12156.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="bib_sparl" style="display: none; width: 80%;" rows="8">@article{cautiousRL,
title={Cautious Reinforcement Learning with Logical Constraints},
author={Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
booktitle={Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
year={2020},
organization={International Foundation for Autonomous Agents and Multiagent Systems}
}</textarea>
									
									<li><b>&bull; <u> Hasanbeig, M. </u>, Jeppu, N. Y., Abate, A., Melham, T., Kroening, D., "<u>DeepSynth: Program Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning</u>", CoRR abs/1911.10244, 2019. <a href="javascript:hide_f('bib_deepsynth')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://arxiv.org/pdf/1911.10244.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="bib_deepsynth" style="display: none; width: 80%;" rows="7">@article{deepsynth,
title={DeepSynth: Program Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning},
author={Hasanbeig, Mohammadhosein and Yogananda Jeppu, Natasha and Abate, Alessandro and Melham, Tom and Kroening, Daniel},
journal={arXiv preprint arXiv:1911.10244},
year={2019}
}</textarea>
									<li><b>&bull; <u> Hasanbeig, M. </u>, Kantaros, Y., Abate, A., Kroening, D., Pappas, G. J., and Lee, I., "<u>Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees</u>", IEEE Conference on Decision and Control, 2019. <a href="javascript:hide_f('bib_plmdp')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://arxiv.org/pdf/1909.05304.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="bib_plmdp" style="display: none; width: 80%;" rows="7">@article{plmdp,
title={Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees},
author={Hasanbeig, Mohammadhosein and Kantaros, Yiannis and Abate, Alessandro and Kroening, Daniel and Pappas, George J. and Lee, Insup},
journal={arXiv preprint arXiv:1909.05304},
year={2019}
}</textarea>
									<li><b>&bull; Lim Zun Yuan, <u> Hasanbeig, M. </u>, Abate, A. and Kroening, D., "<u>Modular Deep Reinforcement Learning with Temporal Logic Specifications</u>", CoRR abs/1909.11591, 2019. <a href="javascript:hide_f('deeplcrl')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://arxiv.org/pdf/1909.11591.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="deeplcrl" style="display: none; width: 80%;" rows="6">@article{deeplcrl,
title={Modular Deep Reinforcement Learning with Temporal Logic Specifications},
author={Yuan, Lim Zun and Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
journal={arXiv preprint arXiv:1909.11591},
year={2019}
}</textarea>
									<li><b>&bull; <u> Hasanbeig, M. </u>, Abate, A. and Kroening, D., "<u>Certified Reinforcement Learning with Logic Guidance</u>", CoRR abs/1902.00778, 2019. <a href="javascript:hide_f('bib_lcrl_j')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://arxiv.org/pdf/1902.00778.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="bib_lcrl_j" style="display: none; width: 80%;" rows="6">@article{lcrl_j,
title={Certified reinforcement learning with logic guidance},
author={Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
journal={arXiv preprint arXiv:1902.00778},
year={2019}
}</textarea>
									<li><b>&bull; <u> Hasanbeig, M. </u>, Abate, A. and Kroening, D., "<u>Logically-Constrained Neural Fitted Q-Iteration</u>", International Conference on Autonomous Agents and Multi-agent Systems, 2019. <a href="javascript:hide_f('bib_lcnfq')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://arxiv.org/pdf/1809.07823.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="bib_lcnfq" style="display: none; width: 80%;" rows="8">@inproceedings{lcnfq,
title={Logically-Constrained Neural Fitted Q-Iteration},
author={Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages={2012--2014},
year={2019},
organization={International Foundation for Autonomous Agents and Multiagent Systems}
}</textarea>
									<li><b>&bull; <u> Hasanbeig, M. </u>, Abate, A. and Kroening, D., "<u>Logically-Constrained Reinforcement Learning</u>", CoRR abs/1801.08099, 2018. <a href="javascript:hide_f('bib_lcrl')">[Bib <i class="far fa-clone"></i>]</a> <a href="https://arxiv.org/pdf/1801.08099.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></b></li>
									<textarea onClick="this.select();" id="bib_lcrl" style="display: none; width: 80%;" rows="6">@article{lcrl,
title={Logically-constrained reinforcement learning},
author={Hasanbeig, Mohammadhosein and Abate, Alessandro and Kroening, Daniel},
journal={arXiv preprint arXiv:1801.08099},
year={2018}
}</textarea>
								</ul>
						</section>
						
						<br>
						
						<ul class="alt">
						<h3>Experiments</h3>
							<h4>"Reinforcement Learning for Temporal Logic Control Synthesis with Probabilistic Satisfaction Guarantees" <a href="https://arxiv.org/pdf/1909.05304.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></h4>
							<h5>- Pacman with Probabilistic Labels</h5>
								<video controls width=43.7%>
								   <source src="downloads\Pacman.mp4#t=0.1" type="video/mp4">
								</video>
							<br><br><div style="border-top: 3px solid green"></div><br>
							<h4>"Logically-Constrained Reinforcement Learning" <a href="https://arxiv.org/pdf/1801.08099.pdf" target="_blank">[PDF <i class="fas fa-external-link-alt"></i>]</a></h4>
							<h5>- Pacman with Deterministic Labels</h5>
							<section>
								<video controls preload="none" poster="downloads\anim5.png" width=43.7%>
								   <source src="downloads\anim5.mp4" type="video/mp4">
								</video>
								<span style="display:inline-block; width: 0.8%;"></span>
								<video controls preload="none" poster="downloads\anim6.png" width=43.7%>
								   <source src="downloads\anim6.mp4" type="video/mp4">
								</video>
								
								<span style="display:inline-block; width: 0.6%;"></span>
								<img src="downloads\lcrl_pacman.png" alt="LCQL converges" width=43.7%>
								<span style="display:inline-block; width: 0.8%;"></span>
								<img src="downloads\rl_pacman.png" alt="Vanilla RL fails to converge" width=45%>
							</section>
							<br>
							<h5>- Slippery Grid-world</h5>
							<section>
								<video controls preload="none" poster="downloads\anim4p.png" width=45%>
								   <source src="downloads\anim4.mp4" type="video/mp4">
								</video>
								<video controls preload="none" poster="downloads\anim3p.png" width=45%>
								   <source src="downloads\anim3.mp4" type="video/mp4">
								</video>
							</section>
						</ul>
					</div>
				</div>

			<!-- Footer -->
				<footer id="footer">
					<h4> hosein.hasanbeig@icloud.com </h4>
					<ul class="icons">
						<li><a href="mailto:hosein.hasanbeig@icloud.com" class="icon fa-envelope"><span class="label">eMail</span></a></li>
						<li><a href="https://www.github.com/grockious" target="_blank" class="icon fa-github"><span class="label">Github</span></a></li>
						<li><a href="https://www.linkedin.com/in/grockious" target="_blank" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://twitter.com/grockious" target="_blank" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://scholar.google.co.uk/citations?user=y0w3o24AAAAJ&hl=en" target="_blank" class="icon fa-graduation-cap"><span class="label">GoogleScholar</span></a></li>
					</ul>
					<ul class="copyright">
						<li>Copyright &copy; Hosein Hasanbeig, <script>new Date().getFullYear()>2017&&document.write(new Date().getFullYear());</script></li>
						<br>
						<iframe height="80%"></iframe>
					</ul>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>
			<script>
				function hide_f(X) {
				  var x = document.getElementById(X);
				  if (x.style.display === "none") {
				    x.style.display = "block";
				  } else {
				    x.style.display = "none";
				  }
				}
			</script>
	</body>
</html>
